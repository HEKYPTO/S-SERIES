{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "ENGLISH_FREQS = [\n",
    "    0.08167, 0.01492, 0.02782, 0.04253, 0.12702, 0.02228, 0.02015,  # A-G\n",
    "    0.06094, 0.06966, 0.00153, 0.00772, 0.04025, 0.02406, 0.06749,  # H-N\n",
    "    0.07507, 0.01929, 0.00095, 0.05987, 0.06327, 0.09056, 0.02758,  # O-U\n",
    "    0.00978, 0.02360, 0.00150, 0.01974, 0.00074                     # V-Z\n",
    "]\n",
    "\n",
    "TWO_LEN_WORD = [\n",
    "    \"as\", \"at\", \"be\", \"he\", \"if\", \"in\", \n",
    "    \"is\", \"it\", \"of\", \"to\"\n",
    "]\n",
    "\n",
    "THREE_LEN_WORD = [\n",
    "    \"and\", \"are\", \"but\", \"for\", \"get\", \n",
    "    \"not\", \"one\", \"the\", \"too\", \"was\"\n",
    "]\n",
    "\n",
    "COMMON_WORD = TWO_LEN_WORD + THREE_LEN_WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{word for word in set(cipher.split(\" \")) if 2 <= len(word) <= 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "all_words = TWO_LEN_WORD + THREE_LEN_WORD\n",
    "combinations = list(itertools.product(all_words, repeat=len(cipher)))\n",
    "\n",
    "all_substitutions = []\n",
    "for combo in combinations:\n",
    "    mapping = dict(zip(cipher_words, combo))\n",
    "    substituted_words = {mapping[word] for word in cipher_words}\n",
    "    all_substitutions.append(substituted_words)\n",
    "\n",
    "for i, substitution in enumerate(all_substitutions):\n",
    "    print(f\"Combination {i + 1}: {substitution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_vigenere(ciphertext):\n",
    "    cleaned_text = ''.join(filter(str.isalpha, ciphertext.upper()))\n",
    "    key_length = guess_key_length(cleaned_text)\n",
    "    key = find_key(cleaned_text, key_length)\n",
    "    return decrypt(ciphertext, key)\n",
    "\n",
    "def guess_key_length(text, max_length=20):\n",
    "    def index_of_coincidence(step):\n",
    "        subtext = text[::step]\n",
    "        n = len(subtext)\n",
    "        freqs = Counter(subtext)\n",
    "        return sum(count * (count - 1) for count in freqs.values()) / (n * (n - 1))\n",
    "\n",
    "    return max(range(1, max_length + 1), key=index_of_coincidence)\n",
    "\n",
    "def find_key(text, key_length):\n",
    "    def find_shift(column):\n",
    "        freqs = Counter(column)\n",
    "        chi_squares = [sum((freqs.get(chr((i + shift) % 26 + 65), 0) / len(column) - ENGLISH_FREQS[i]) ** 2 / ENGLISH_FREQS[i]\n",
    "                           for i in range(26)) for shift in range(26)]\n",
    "        return chi_squares.index(min(chi_squares))\n",
    "\n",
    "    return ''.join(chr(find_shift(text[i::key_length]) + 65) for i in range(key_length))\n",
    "\n",
    "def decrypt(ciphertext, key):\n",
    "    return ''.join(chr((ord(c) - ord(key[i % len(key)]) + 26) % 26 + 65) if c.isalpha() else c\n",
    "                   for i, c in enumerate(ciphertext.upper()))\n",
    "\n",
    "def improve_decryption(decrypted, max_iterations=1000):\n",
    "    best_score = float('inf')\n",
    "    best_text = decrypted\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        improved = best_text\n",
    "        for word in COMMON_WORDS:\n",
    "            pattern = r'\\b' + '.' * len(word) + r'\\b'\n",
    "            improved = re.sub(pattern, word, improved, count=1)\n",
    "\n",
    "        score = score_text(improved)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_text = improved\n",
    "        else:\n",
    "            break  # Stop if no improvement\n",
    "\n",
    "    return best_text\n",
    "\n",
    "def score_text(text):\n",
    "    words = text.split()\n",
    "    unknown_words = sum(1 for word in words if word not in COMMON_WORDS)\n",
    "    return unknown_words / len(words)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    decrypted = decrypt_vigenere(ciphertext)\n",
    "    print(f\"Initial decryption: {decrypted}\")\n",
    "    improved = improve_decryption(decrypted)\n",
    "    print(f\"Improved decryption: {improved}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download('words')\n",
    "\n",
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZKMOLTROE.\"\n",
    "\n",
    "cipher_words = cipher.replace('.', '').split()\n",
    "\n",
    "nltk_words = set(words.words())\n",
    "\n",
    "def word_pattern(word):\n",
    "    pattern = {}\n",
    "    return [pattern.setdefault(char, len(pattern)) for char in word]\n",
    "\n",
    "def find_matching_words(cipher_word):\n",
    "    cipher_pattern = word_pattern(cipher_word)\n",
    "    matches = [word for word in nltk_words if len(word) == len(cipher_word) and word_pattern(word.upper()) == cipher_pattern]\n",
    "    return matches\n",
    "\n",
    "for cipher_word in cipher_words:\n",
    "    matches = find_matching_words(cipher_word)\n",
    "    print(f\"Cipher word '{cipher_word}' matches: {matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "\n",
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\"\n",
    "\n",
    "cipher_words = cipher.replace('.', '').split()\n",
    "\n",
    "nltk_words = set(word.lower() for word in words.words())\n",
    "\n",
    "from nltk.corpus import brown\n",
    "word_freq = FreqDist([word.lower() for word in brown.words()])\n",
    "\n",
    "def word_pattern(word):\n",
    "    pattern = {}\n",
    "    return [pattern.setdefault(char, len(pattern)) for char in word.lower()]\n",
    "\n",
    "\n",
    "def find_matching_words(cipher_word):\n",
    "    cipher_pattern = word_pattern(cipher_word)\n",
    "    matches = [word for word in nltk_words if len(word) == len(cipher_word) and word_pattern(word) == cipher_pattern]\n",
    "    sorted_matches = sorted(matches, key=lambda x: word_freq[x], reverse=True)\n",
    "    return sorted_matches\n",
    "\n",
    "\n",
    "for cipher_word in cipher_words:\n",
    "    matches = find_matching_words(cipher_word)\n",
    "    print(f\"Cipher word '{cipher_word}' matches: {matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "\n",
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\"\n",
    "\n",
    "cipher_words = cipher.replace('.', '').split()\n",
    "nltk_words = set(word.lower() for word in words.words())\n",
    "\n",
    "from nltk.corpus import brown\n",
    "word_freq = FreqDist([word.lower() for word in brown.words()])\n",
    "\n",
    "def create_pattern(substitution_pattern):\n",
    "    return ''.join('*' if char.isupper() else char for char in substitution_pattern)\n",
    "\n",
    "def find_matching_words(substitution_pattern):\n",
    "    pattern = create_pattern(substitution_pattern)\n",
    "    \n",
    "    regex_pattern = '^' + pattern.replace('*', '.') + '$' \n",
    "    matches = [word for word in nltk_words if re.fullmatch(regex_pattern, word)]\n",
    "    \n",
    "    sorted_matches = sorted(matches, key=lambda x: word_freq[x], reverse=True)\n",
    "    return sorted_matches\n",
    "\n",
    "substitution_pattern = 'LIU'  \n",
    "\n",
    "matches = find_matching_words(substitution_pattern)\n",
    "print(f\"Cipher with pattern '{substitution_pattern}' matches: {matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_vigrene(text: str):\n",
    "    chunks = sorted(set(text.replace(\".\", \"\").split(\" \")), key=len)[::-1]\n",
    "    \n",
    "    def iterate_subs(chunk: list[str], index: int):\n",
    "        \n",
    "        print(chunk, index)\n",
    "        \n",
    "        if \"\".join(chunk).islower():\n",
    "            print(chunk)\n",
    "            return chunk\n",
    "        \n",
    "        if (chunk[index].islower()):\n",
    "            iterate_subs(chunk, index + 1)\n",
    "        \n",
    "        matches = find_matching_words(chunk[index])\n",
    "        if len(matches) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        for match in matches:\n",
    "            if match in chunk:\n",
    "                continue\n",
    "            \n",
    "            subbed_chunk = []\n",
    "            for text in chunk:\n",
    "                sub_text = text\n",
    "                for i in range(len(match)):\n",
    "                    sub_text = sub_text.replace(chunk[index][i], match[i])\n",
    "                    \n",
    "                subbed_chunk.append(sub_text)\n",
    "                \n",
    "            candidate_answer = iterate_subs(subbed_chunk, index + 1)\n",
    "            if candidate_answer == \"\":\n",
    "                continue\n",
    "                \n",
    "    iterate_subs(chunks, 0)\n",
    "                    \n",
    "    \n",
    "solve_vigrene(cipher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = ['the', 'and', 'was', 'for', 'his', 'had', 'not', 'are', 'but', 'one', 'you', 'her', 'all', 'she', 'him', 'who', 'out', 'its', 'can', 'new', 'two', 'may', 'any', 'now', 'our', 'man', 'did', 'way', 'how', 'too', 'see', 'own', 'men', 'get', 'day', 'old', 'off', 'few', 'use', 'say', 'got', 'war', 'put', 'far', 'yet', 'set', 'end', 'why', 'let', 'per', 'big', 'saw', 'god', 'law', 'act', 'car', 'air', 'ago', 'boy', 'job', 'age', 'six', 'run', 'art', 'top', 'tax', 'red', 'nor', 'cut', 'low', 'pay', 'son', 'ten', 'sat', 'yes', 'bad', 'due', 'try', 'lay', 'aid', 'ran', 'led', 'met', 'hot', 'ask', 'bed', 'lot', 'eye', 'gun', 'hit', 'sun', 'bit', 'gas', 'sea', 'sir', 'oil', 'arm', 'key', 'add', 'sex', 'bar', 'sam', 'fit', 'dog', 'die', 'san', 'buy', 'box', 'dry', 'won', 'sit', 'tom', 'eat', 'fat', 'lie', 'sky', 'leg', 'bay', 'hat', 'joe', 'win', 'sin', 'wet', 'guy', 'via', 'cry', 'cup', 'ice', 'sum', 'fun', 'odd', 'raw', 'bag', 'arc', 'fed', 'joy', 'bob', 'mad', 'aim', 'roy', 'jim', 'row', 'pat', 'sad', 'net', 'bus', 'lee', 'fly', 'mud', 'van', 'gay', 'era', 'ann', 'ear', 'tea', 'cow', 'jet', 'pot', 'hal', 'cap', 'dan', 'ill', 'zen', 'jew', 'fog', 'tim', 'don', 'tie', 'gin', 'cat', 'pip', 'bid', 'tip', 'hen', 'ben', 'kay', 'shu', 'wit', 'doc', 'bet', 'ray', 'hay', 'ham', 'rid', 'dim', 'lid', 'rob', 'lap', 'eve', 'fan', 'pen', 'sue', 'bat', 'lip', 'rod', 'tap', 'lao', 'gap', 'ada', 'bod', 'lit', 'pin', 'pro', 'jar', 'ate', 'pan', 'jaw', 'fee', 'mae', 'ace', 'dug', 'oak', 'jay', 'cop', 'amy', 'dad', 'bow', 'nut', 'hey', 'apt', 'pie', 'gum', 'fix', 'wax', 'pit', 'ego', 'fur', 'rug', 'hut', 'par', 'mix', 'shy', 'nam', 'map', 'tub', 'lou', 'ton', 'dot', 'thy', 'cab', 'tar', 'icy', 'fox', 'max', 'tin', 'ken', 'egg', 'nod', 'log', 'del', 'rev', 'beg', 'ash', 'quo', 'bee', 'hub', 'rag', 'dig', 'hip', 'owe', 'non', 'mob', 'spy', 'ivy', 'zoo', 'inn', 'tan', 'bin', 'toe', 'han', 'bud', 'jed', 'mel', 'yin', 'sac', 'pad', 'flu', 'rot', 'foe', 'pet', 'pam', 'pig', 'jig', 'pop', 'leo', 'ted', 'con', 'mao', 'ban', 'ink', 'bum', 'boa', 'dag', 'axe', 'jam', 'ant', 'sew', 'dip', 'sax', 'ion', 'bey', 'rub', 'rue', 'rip', 'gyp', 'pry', 'jug', 'lad', 'web', 'cod', 'rat', 'hum', 'rig', 'ass', 'tee', 'sub', 'coe', 'huh', 'abo', 'ole', 'las', 'yow', 'cox', 'aft', 'awe', 'woe', 'rex', 'sly', 'gal', 'mat', 'rim', 'tag', 'min', 'ski', 'wry', 'poe', 'hem', 'mar', 'gee', 'gem', 'gag', 'wee', 'dam', 'eta', 'toy', 'nap', 'bug', 'tao', 'sag', 'ron', 'ike', 'rio', 'ado', 'bel', 'coy', 'bam', 'lew', 'git', 'jan', 'pah', 'dow', 'peg', 'vic', 'rye', 'yen', 'nip', 'lab', 'sis', 'bop', 'sur', 'ape', 'tug', 'lax', 'mid', 'les', 'mop', 'lag', 'abe', 'woo', 'sow', 'hug', 'elm', 'rum', 'len', 'gus', 'hog', 'dew', 'pod', 'sol', 'sod', 'paw', 'yea', 'ory', 'pee', 'eel', 'nun', 'sip', 'och', 'maw', 'urn', 'kit', 'lex', 'kin', 'hun', 'yuh', 'fin', 'spa', 'gym', 'qua', 'nab', 'fig', 'reb', 'foy', 'biz', 'eli', 'wed', 'sic', 'lug', 'gel', 'bye', 'fir', 'pal', 'psi', 'fad', 'pup', 'das', 'phi', 'bea', 'meg', 'bon', 'rap', 'ram', 'den', 'owl', 'ida', 'wei', 'hop', 'hon', 'dey', 'ira', 'nay', 'hex', 'nan', 'wan', 'alf', 'keg', 'mah', 'fry', 'bib', 'vow', 'ani', 'jon', 'sap', 'mot', 'nat', 'wes', 'pap', 'nob', 'mor', 'aye', 'cud', 'bah', 'tai', 'gog', 'ist', 'wig', 'ana', 'soy', 'cal', 'zip', 'yon', 'hob', 'moi', 'mew', 'mon', 'din', 'sid', 'gab', 'gil', 'gig', 'tow', 'vet', 'mem', 'tab', 'wow', 'lev', 'ire', 'hap', 'eva', 'wop', 'mig', 'ito', 'oft', 'amt', 'fra', 'wac', 'bun', 'mag', 'rok', 'lop', 'cur', 'ich', 'mum', 'aku', 'kob', 'bal', 'wod', 'gob', 'sou', 'tat', 'ade', 'mug', 'nae', 'pax', 'ewe', 'soe', 'haw', 'jab', 'lob', 'pow', 'sop', 'ind', 'goa', 'err', 'cam', 'ama', 'obe', 'elk', 'vex', 'rut', 'jot', 'hue', 'jag', 'pub', 'mal', 'bas', 'naw', 'nil', 'ere', 'gut', 'roi', 'tau', 'chi', 'dun', 'ova', 'lui', 'roe', 'rib', 'ale', 'doe', 'ebb', 'boo', 'ell', 'hoy', 'orb', 'dos', 'bog', 'fay', 'mac', 'che', 'liz', 'sup', 'wyn', 'mrs', 'sie', 'yip', 'pun', 'dud', 'ugh', 'loy', 'lak', 'fob', 'iyo', 'ges', 'aht', 'pur', 'zar', 'sil', 'yun', 'kop', 'gup', 'bur', 'mib', 'ino', 'ing', 'ret', 'gey', 'kip', 'waw', 'tol', 'hwa', 'reg', 'syd', 'kee', 'hox', 'neb', 'gor', 'uro', 'cum', 'aba', 'tew', 'kos', 'tib', 'mou', 'gim', 'tur', 'awa', 'kea', 'ore', 'mer', 'bub', 'kyl', 'eon', 'yer', 'mes', 'kyu', 'ods', 'dop', 'ers', 'emu', 'ura', 'vag', 'twa', 'yak', 'lai', 'vim', 'tae', 'ref', 'lye', 'wut', 'saj', 'hud', 'ume', 'ree', 'ket', 'oes', 'rit', 'ait', 'mod', 'hia', 'lis', 'rab', 'ail', 'kua', 'lan', 'oar', 'pic', 'ahu', 'tyt', 'eyn', 'deg', 'fey', 'col', 'yep', 'pon', 'nth', 'ped', 'mam', 'oxy', 'hic', 'wog', 'loo', 'vip', 'wot', 'fro', 'pix', 'aum', 'ras', 'pyx', 'emm', 'coo', 'fod', 'nep', 'fez', 'ism', 'foo', 'oki', 'nou', 'fut', 'cos', 'ark', 'aly', 'pac', 'kan', 'ort', 'voe', 'lam', 'ast', 'ssi', 'ger', 'yor', 'hag', 'sob', 'pep', 'hin', 'mwa', 'nib', 'owk', 'aga', 'ria', 'luc', 'fud', 'vau', 'gen', 'tst', 'lux', 'sla', 'aln', 'dib', 'cog', 'fot', 'iba', 'abb', 'dha', 'ila', 'pho', 'zel', 'ged', 'toa', 'dae', 'obi', 'tck', 'dis', 'hoi', 'pst', 'tua', 'sec', 'pim', 'teg', 'feu', 'wup', 'jun', 'fae', 'rah', 'aix', 'zat', 'lek', 'reh', 'cig', 'cue', 'gip', 'gau', 'ona', 'gnu', 'vis', 'ush', 'gan', 'sov', 'pol', 'mim', 'wob', 'jos', 'gio', 'sia', 'dub', 'wem', 'wae', 'poy', 'sab', 'blo', 'tch', 'ala', 'wen', 'fag', 'oii', 'zac', 'tou', 'hao', 'flo', 'bra', 'taa', 'coz', 'yaw', 'gif', 'yok', 'jur', 'tji', 'dar', 'aby', 'wim', 'jut', 'pyr', 'neo', 'nee', 'cro', 'goo', 'geo', 'gat', 'hah', 'rud', 'hew', 'lac', 'sri', 'poi', 'koi', 'azo', 'eld', 'wiz', 'pes', 'yus', 'kaw', 'bes', 'tho', 'oda', 'ned', 'tog', 'aam', 'poa', 'seg', 'mho', 'lim', 'gaw', 'suu', 'tal', 'osc', 'udo', 'twi', 'duo', 'baw', 'kha', 'sus', 'sal', 'oat', 'yex', 'alt', 'ita', 'div', 'baa', 'khu', 'edo', 'odz', 'lat', 'laz', 'ked', 'dee', 'dak', 'tri', 'sty', 'sma', 'uds', 'roc', 'dob', 'swa', 'ima', 'wer', 'cor', 'grr', 'tam', 'fon', 'uca', 'wun', 'wab', 'yot', 'mil', 'ihi', 'ule', 'jog', 'zea', 'tox', 'awl', 'ibo', 'huk', 'nul', 'stu', 'lif', 'iao', 'fam', 'rik', 'mog', 'toi', 'ase', 'caw', 'pul', 'ula', 'cho', 'tay', 'zer', 'owd', 'yis', 'kou', 'vie', 'zep', 'dal', 'vat', 'alk', 'moy', 'pug', 'fop', 'gra', 'oho', 'ilk', 'ons', 'asp', 'oka', 'sim', 'dux', 'yox', 'noy', 'bab', 'fow', 'iso', 'goy', 'pua', 'fib', 'mus', 'kil', 'tup', 'irk', 'ley', 'keb', 'alp', 'jat', 'yah', 'sud', 'yoy', 'suz', 'tad', 'sny', 'lut', 'mas', 'ora', 'ull', 'ami', 'ake', 'zag', 'ust', 'lin', 'kat', 'urd', 'fou', 'wur', 'tez', 'rox', 'lwo', 'dao', 'ara', 'sog', 'ata', 'lea', 'edh', 'aha', 'utu', 'yam', 'aer', 'lod', 'tra', 'pom', 'gid', 'elf', 'moo', 'rep', 'iva', 'wha', 'tun', 'sah', 'wro', 'eer', 'dah', 'kef', 'nim', 'oam', 'jib', 'kui', 'iwa', 'ply', 'pir', 'gib', 'leu', 'rad', 'taw', 'vog', 'raj', 'nix', 'cid', 'tha', 'pik', 'soh', 'wey', 'hup', 'fei', 'cha', 'urf', 'ssu', 'deb', 'pud', 'kim', 'adz', 'jem', 'goi', 'fet', 'rix', 'arx', 'opt', 'nei', 'wah', 'aro', 'gez', 'agy', 'bae', 'aho', 'naa', 'pus', 'sok', 'yas', 'rhe', 'vod', 'gue', 'dap', 'zed', 'cag', 'rog', 'rho', 'bis', 'ulu', 'auk', 'vei', 'wye', 'cyp', 'mya', 'ean', 'jin', 'ido', 'lar', 'ava', 'uva', 'tyg', 'zig', 'fen', 'rax', 'koa', 'sye', 'elb', 'liv', 'aru', 'yao', 'dod', 'cep', 'wud', 'taj', 'uta', 'soc', 'hak', 'udi', 'moe', 'pya', 'abu', 'imu', 'ame', 'jef', 'nea', 'kai', 'dev', 'wid', 'ife', 'bap', 'imp', 'meo', 'gur', 'loa', 'sak', 'nub', 'ach', 'vol', 'ide', 'ako', 'tor', 'zoa', 'ode', 'guz', 'tui', 'gad', 'ofo', 'sho', 'zan', 'mau', 'til', 'yap', 'ump', 'lox', 'bom', 'noa', 'tum', 'ess', 'cwm', 'hau', 'tot', 'yed', 'dom', 'tid', 'yan', 'cay', 'auh', 'bor', 'imi', 'eke', 'wis', 'ope', 'kor', 'sen', 'nye', 'dhu', 'mir', 'ijo', 'dup', 'rus', 'rux', 'sot', 'cub', 'uji', 'lue', 'tig', 'gos', 'ock', 'fid', 'gon', 'orf', 'rie', 'vum', 'yeo', 'ser', 'una', 'vas', 'phu', 'dor', 'nef', 'sib', 'yee', 'daw', 'ute', 'uke', 'sar', 'zee', 'alb', 'bim', 'pau', 'rea', 'ure', 'pea', 'avo', 'lur', 'hod', 'wap', 'kaj', 'pia', 'nid', 'arn', 'kon', 'wea', 'yid', 'apa', 'lof', 'yew', 'ose', 'hei', 'ubi', 'yez', 'ker', 'ler', 'jow', 'ayu', 'sig', 'urs', 'nag', 'wag', 'gul', 'elt', 'val', 'dum', 'kol', 'eft', 'uru', 'mru', 'tue', 'oaf', 'zax', 'hep', 'wat', 'cob', 'ber', 'sai', 'mow', 'suk', 'woy', 'nig', 'kex', 'pox', 'wir', 'gaj', 'sey', 'aes', 'nit', 'ohm', 'els', 'jud', 'ric', 'bot', 'ary', 'fip', 'yoi', 'orc', 'awn', 'aka', 'hie', 'gud', 'tic', 'ian', 'jap', 'cot', 'dit', 'ens', 'tye', 'erd', 'gar', 'kra', 'rel', 'ouf', 'mev', 'ati', 'kaf', 'oer', 'gaz', 'ave', 'fub', 'eme', 'cit', 'nog', 'vai', 'hui', 'fie', 'yad', 'vug', 'pew', 'sao', 'mab', 'yar', 'dab', 'uri', 'nar', 'umu', 'mux', 'aal', 'olm', 'gol', 'het', 'ady', 'sha', 'dye', 'nak', 'gie', 'shi', 'waf', 'lys', 'cee', 'alo', 'gam', 'kru', 'tod', 'mun', 'upo', 'dol', 'bos', 'awd', 'hsi', 'oto', 'zak', 'lei', 'yat', 'luo', 'tut', 'lum', 'hyp', 'cly', 'wad', 'tit', 'hoe', 'asa', 'fum', 'poh', 'tav', 'pob', 'bac', 'tux', 'yoe', 'tec', 'yom', 'sui', 'saa', 'erg', 'rua', 'aus', 'cad', 'kep', 'vee', 'zad', 'gyn']\n",
    "print(\"old\" in match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('reuters')\n",
    "bigrams = list(ngrams(reuters.words(), 2))\n",
    "bigram_freq = nltk.FreqDist(bigrams)\n",
    "\n",
    "def evaluate_decryption(decrypted_message):\n",
    "    words = decrypted_message.split()\n",
    "    message_bigrams = list(ngrams(words, 2))\n",
    "    score = sum(bigram_freq[bigram] for bigram in message_bigrams)\n",
    "    return score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\"\n",
    "answer = \"SECURITY IS THE FIRST CAUSE OF MISFORTUNE. THIS IS AN OLD GERMAN PROVERB.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_decryption('security is the first cause of misfortune. this is an old german proverb.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown\n",
    "import re\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "nltk_words = set(word.lower() for word in words.words())\n",
    "\n",
    "word_freq = FreqDist([word.lower() for word in brown.words()])\n",
    "\n",
    "def create_pattern(substitution_pattern):\n",
    "    return ''.join('*' if char.isupper() else char for char in substitution_pattern)\n",
    "\n",
    "def find_matching_words(substitution_pattern):\n",
    "    pattern = create_pattern(substitution_pattern)\n",
    "    \n",
    "    regex_pattern = '^' + pattern.replace('*', '.') + '$' \n",
    "    matches = [word for word in nltk_words if re.fullmatch(regex_pattern, word)]\n",
    "    \n",
    "    sorted_matches = sorted(matches, key=lambda x: word_freq[x], reverse=True)\n",
    "    return sorted_matches\n",
    "\n",
    "\n",
    "nltk.download('reuters')\n",
    "bigrams = list(ngrams(reuters.words(), 2))\n",
    "bigram_freq = nltk.FreqDist(bigrams)\n",
    "\n",
    "def evaluate_decryption(decrypted_message):\n",
    "    words = decrypted_message.split()\n",
    "    message_bigrams = list(ngrams(words, 2))\n",
    "    score = sum(bigram_freq[bigram] for bigram in message_bigrams)\n",
    "    return score  \n",
    "\n",
    "\n",
    "def solve_vigrene(text: str):\n",
    "    chunks = sorted(set(text.replace(\".\", \"\").split(\" \")), key=len, reverse=True)\n",
    "    \n",
    "    def iterate_subs(chunk: list[str], index: int) -> list[str]:\n",
    "        nonlocal chunks, text\n",
    "        \n",
    "        print(chunks)\n",
    "        print(chunk)\n",
    "        \n",
    "        possible_answers = []\n",
    "\n",
    "        if all(word.islower() for word in chunk):\n",
    "            copy_text = text  \n",
    "            for i in range(len(chunk)):\n",
    "                copy_text = copy_text.replace(chunks[i], chunk[i])  \n",
    "                \n",
    "            possible_answers.append(copy_text)\n",
    "            return possible_answers\n",
    "        \n",
    "        if chunk[index].islower():\n",
    "            return iterate_subs(chunk, index + 1)\n",
    "        \n",
    "        matches = find_matching_words(chunk[index])\n",
    "        if not matches:\n",
    "            return []\n",
    "            \n",
    "            subbed_chunk = []\n",
    "            for word in chunk:\n",
    "                sub_text = word\n",
    "                for i in range(len(match)):\n",
    "                    sub_text = sub_text.replace(chunk[index][i], match[i])\n",
    "                subbed_chunk.append(sub_text)\n",
    "            \n",
    "            candidate_answers = iterate_subs(subbed_chunk, index + 1)\n",
    "            possible_answers.extend(candidate_answers)\n",
    "        \n",
    "        return possible_answers\n",
    "    \n",
    "    possible_texts = iterate_subs(chunks, 0)\n",
    "    print(possible_texts)\n",
    "    best_text = max(possible_texts, key=lambda x: evaluate_decryption(x))\n",
    "    print(f\"Best decrypted text: {best_text}\")\n",
    "\n",
    "\n",
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\"\n",
    "\n",
    "solve_vigrene(cipher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(strings: list[str], filename: str):\n",
    "    with open(filename, 'w') as file:\n",
    "        for line in strings:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "\n",
    "filename = \"output.txt\"\n",
    "write_list_to_file(dicipher, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import words as nltk_words\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "\n",
    "\n",
    "english_vocab = set(nltk_words.words())\n",
    "\n",
    "\n",
    "bigram_freq = FreqDist(nltk.bigrams(nltk.corpus.brown.words()))\n",
    "\n",
    "def evaluate_decryption(decrypted_message):\n",
    "    \n",
    "    words = decrypted_message.split()\n",
    "\n",
    "    \n",
    "    english_word_count = sum(1 for word in words if word.lower().strip(string.punctuation) in english_vocab)\n",
    "    english_word_ratio = english_word_count / len(words) if words else 0\n",
    "\n",
    "    \n",
    "    message_bigrams = list(ngrams(words, 2))\n",
    "    bigram_score = sum(bigram_freq[bigram] for bigram in message_bigrams)\n",
    "\n",
    "    \n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "    \n",
    "    score = english_word_ratio * 100 + bigram_score * 0.01 - abs(4.5 - avg_word_length) * 10\n",
    "    \n",
    "    return score, english_word_ratio, avg_word_length, bigram_score\n",
    "\n",
    "\n",
    "decrypted_message = \"This is a sample decoded message to test the function.\"\n",
    "score, english_word_ratio, avg_word_length, bigram_score = evaluate_decryption(decrypted_message)\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"English Word Ratio: {english_word_ratio}\")\n",
    "print(f\"Average Word Length: {avg_word_length}\")\n",
    "print(f\"Bigram Score: {bigram_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Assuming evaluate_decryption is already defined as shown earlier\n",
    "\n",
    "def find_best_sentence(filename: str):\n",
    "    with open(filename, 'r') as file:\n",
    "        sentences = file.readlines()\n",
    "    \n",
    "    best_sentence = None\n",
    "    best_bigram_score = float('-inf')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        score, english_word_ratio, avg_word_length, bigram_score = evaluate_decryption(sentence.strip())\n",
    "        print(f\"Evaluating: {sentence.strip()}\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"English Word Ratio: {english_word_ratio}\")\n",
    "        print(f\"Average Word Length: {avg_word_length}\")\n",
    "        print(f\"Bigram Score: {bigram_score}\\n\")\n",
    "        \n",
    "        if bigram_score > best_bigram_score and english_word_ratio == 1:\n",
    "            best_bigram_score = bigram_score\n",
    "            best_sentence = sentence.strip()\n",
    "    \n",
    "    return best_sentence, best_bigram_score\n",
    "\n",
    "# Example usage\n",
    "best_sentence, best_bigram_score = find_best_sentence('output.txt')\n",
    "print(f\"Best Sentence: {best_sentence}\")\n",
    "print(f\"Best Bigram Score: {best_bigram_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import reuters, brown\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "\n",
    "nltk_words = set(word.lower() for word in words.words())\n",
    "word_freq = FreqDist([word.lower() for word in brown.words()])\n",
    "\n",
    "def create_pattern(substitution_pattern):\n",
    "    return ''.join('*' if char.isupper() else char for char in substitution_pattern)\n",
    "\n",
    "def find_matching_words(substitution_pattern):\n",
    "    pattern = create_pattern(substitution_pattern)\n",
    "    regex_pattern = '^' + pattern.replace('*', '.') + '$' \n",
    "    matches = [word for word in nltk_words if re.fullmatch(regex_pattern, word)]\n",
    "    sorted_matches = sorted(matches, key=lambda x: word_freq[x], reverse=True)\n",
    "    return sorted_matches\n",
    "\n",
    "bigrams = list(ngrams(reuters.words(), 2))\n",
    "bigram_freq = FreqDist(bigrams)\n",
    "\n",
    "def evaluate_decryption(decrypted_message):\n",
    "    words = decrypted_message.split()\n",
    "    message_bigrams = list(ngrams(words, 2))\n",
    "    score = sum(bigram_freq[bigram] for bigram in message_bigrams)\n",
    "    return score  \n",
    "\n",
    "def solve_vigrene(text: str):\n",
    "    chunks = sorted(set(text.replace(\".\", \"\").split(\" \")), key=len, reverse=True)\n",
    "    \n",
    "    def iterate_subs(chunk: list[str], index: int) -> list[str]:\n",
    "        nonlocal chunks, text\n",
    "        possible_answers = []\n",
    "\n",
    "        if all(word.islower() for word in chunk):\n",
    "            copy_text = text\n",
    "            for i in range(len(chunk)):\n",
    "                copy_text = copy_text.replace(chunks[i], chunk[i])\n",
    "            possible_answers.append(copy_text)\n",
    "            return possible_answers\n",
    "        \n",
    "        if index >= len(chunk): \n",
    "            return []\n",
    "        \n",
    "        if chunk[index].islower():  \n",
    "            return iterate_subs(chunk, index + 1)\n",
    "        \n",
    "        matches = find_matching_words(chunk[index])\n",
    "        if not matches:\n",
    "            return []\n",
    "        \n",
    "        for match in tqdm(matches, desc=f\"Processing word\"):\n",
    "            if any(c in ''.join(chunk).lower() for c in match):\n",
    "                continue\n",
    "            \n",
    "            subbed_chunk = []\n",
    "            for word in chunk:\n",
    "                sub_text = word\n",
    "                for i in range(min(len(match), len(chunk[index]))):\n",
    "                    sub_text = sub_text.replace(chunk[index][i], match[i])\n",
    "                subbed_chunk.append(sub_text)\n",
    "            \n",
    "            candidate_answers = iterate_subs(subbed_chunk, index + 1)\n",
    "            possible_answers.extend(candidate_answers)\n",
    "        \n",
    "        return possible_answers\n",
    "    \n",
    "    possible_texts = iterate_subs(chunks, 0)\n",
    "    return possible_texts\n",
    "\n",
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\"\n",
    "decrypted_texts = solve_vigrene(cipher)\n",
    "\n",
    "for text in decrypted_texts:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\"\n",
    "answer = \"SECURITY IS THE FIRST CAUSE OF MISFORTUNE. THIS IS AN OLD GERMAN PROVERB.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import reuters, brown\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "\n",
    "nltk_words = set(word.lower() for word in words.words())\n",
    "word_freq = FreqDist([word.lower() for word in brown.words()])\n",
    "\n",
    "def create_pattern(substitution_pattern):\n",
    "    return ''.join('*' if char.isupper() else char for char in substitution_pattern)\n",
    "\n",
    "def find_matching_words(substitution_pattern):\n",
    "    pattern = create_pattern(substitution_pattern)\n",
    "    regex_pattern = '^' + pattern.replace('*', '.') + '$' \n",
    "    matches = [word for word in nltk_words if re.fullmatch(regex_pattern, word)]\n",
    "    sorted_matches = sorted(matches, key=lambda x: word_freq[x], reverse=True)\n",
    "    return sorted_matches\n",
    "\n",
    "bigrams = ngrams(brown.words(), 2)\n",
    "bigram_freq = defaultdict(int)\n",
    "for bigram in bigrams:\n",
    "    bigram_freq[bigram] += 1\n",
    "\n",
    "brown_words_set = set(word.lower() for word in brown.words())\n",
    "frequency_order = 'etaoinshrdlcumfwygpbvkxjqz'[::-1]\n",
    "\n",
    "def evaluate_decryption(decrypted_message):\n",
    "    words = decrypted_message.split()\n",
    "    message_bigrams = list(ngrams(words, 2))\n",
    "    bigram_score = sum(bigram_freq.get(bigram, 0) for bigram in message_bigrams)\n",
    "    english_word_ratio = sum(1 for word in words if word.lower().replace(\".\", \"\") in brown_words_set) / len(words)\n",
    "    char_score = sum(frequency_order.index(char.lower()) for char in decrypted_message if char.lower() in frequency_order)  \n",
    "    return bigram_score, english_word_ratio, char_score\n",
    "\n",
    "def validate_mapping(cipher: list[str], dicipher: list[str]) -> bool:\n",
    "    mapping = dict()\n",
    "    join_cipher = \"\".join(cipher)\n",
    "    join_dicipher = \"\".join(dicipher)\n",
    "    \n",
    "    return len(set(join_cipher)) == len(set(join_dicipher)) # Should be 1-1        \n",
    "    \n",
    "def solve_vigrene(text: str):\n",
    "    chunks = sorted(set(text.replace(\".\", \"\").split(\" \")), key=len, reverse=True)\n",
    "    \n",
    "    def iterate_subs(chunk: list[str], index: int) -> list[str]:\n",
    "        if all(word.islower() for word in chunk):\n",
    "            if not validate_mapping(chunks, chunk):\n",
    "                return []\n",
    "            copy_text = text\n",
    "            for i, word in enumerate(chunk):\n",
    "                copy_text = copy_text.replace(chunks[i], word)\n",
    "            return [copy_text]\n",
    "        \n",
    "        if index >= len(chunk):\n",
    "            return []\n",
    "        \n",
    "        if chunk[index].islower():\n",
    "            return iterate_subs(chunk, index + 1)\n",
    "        \n",
    "        matches = find_matching_words(chunk[index])\n",
    "        if not matches:\n",
    "            return []\n",
    "\n",
    "        possible_answers = []\n",
    "        used_matches = set()\n",
    "        \n",
    "        for match in tqdm(matches, desc=f\"Processing chunk: \"):\n",
    "            if match in used_matches:\n",
    "                continue\n",
    "            \n",
    "            used_matches.add(match)\n",
    "            \n",
    "            subbed_chunk = []\n",
    "            for word in chunk:\n",
    "                sub_text = word\n",
    "                if match:\n",
    "                    sub_text = sub_text.translate(str.maketrans(chunk[index], match))\n",
    "                subbed_chunk.append(sub_text)\n",
    "\n",
    "            possible_answers.extend(iterate_subs(subbed_chunk, index + 1))\n",
    "        \n",
    "        return possible_answers\n",
    "    \n",
    "    possible_texts = iterate_subs(chunks, 0)\n",
    "\n",
    "    best_decryption = None\n",
    "    best_score = None\n",
    "    \n",
    "    for decrypted_message in possible_texts:\n",
    "        bigram_score, english_word_ratio, char_score = evaluate_decryption(decrypted_message)\n",
    "        if english_word_ratio == 1: # ONLY ALLOW ENGLISH\n",
    "            score = bigram_score + char_score  \n",
    "            if best_score is None or score > best_score:\n",
    "                best_score = score\n",
    "                best_decryption = decrypted_message\n",
    "    \n",
    "    return best_decryption\n",
    "\n",
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\"\n",
    "print(solve_vigrene(cipher).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ['security is the first cause of misfortune. this is an old german proverb.', 'security is the first cause of misfortune. this is an owl german proverb.', 'security is the first cause of misfortune. this is an owk german proverb.', 'security is the first cause of misfortune. this is an odz german proverb.', 'security is the first cause of misfortune. this is an owd german proverb.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "\n",
    "bigrams = ngrams(brown.words(), 2)\n",
    "bigram_freq = defaultdict(int)\n",
    "for bigram in bigrams:\n",
    "    bigram_freq[bigram] += 1\n",
    "\n",
    "brown_words_set = set(word.lower() for word in brown.words())\n",
    "frequency_order = 'etaoinshrdlcumfwygpbvkxjqz'[::-1]\n",
    "\n",
    "def evaluate_decryption(decrypted_message):\n",
    "    words = decrypted_message.split()\n",
    "\n",
    "    message_bigrams = list(ngrams(words, 2))\n",
    "    bigram_score = sum(bigram_freq.get(bigram, 0) for bigram in message_bigrams)\n",
    "    \n",
    "    english_word_ratio = sum(1 for word in words if word.lower().replace(\".\", \"\") in brown_words_set) / len(words)\n",
    "    \n",
    "    char_score = sum(frequency_order.index(char.lower()) for char in decrypted_message if char.lower() in frequency_order)\n",
    "    \n",
    "    return bigram_score, english_word_ratio, char_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in w:\n",
    "    print(evaluate_decryption(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluating: security is the first cause of misfortune. this is an old german grocery.\n",
    "Overall Score: 17668.466455918515\n",
    "English Word Ratio: 1.0\n",
    "Average Word Length: 4.6923076923076925\n",
    "Bigram Score: 1757\n",
    "Length Coherence: 0.8666666666666667\n",
    "Most Common Character Frequency: 0.0958904109589041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluating: security is the first cause of misfortune. this is an old german proverb.\n",
    "Overall Score: 17668.466455918515\n",
    "English Word Ratio: 1.0\n",
    "Average Word Length: 4.6923076923076925\n",
    "Bigram Score: 1757\n",
    "Length Coherence: 0.8666666666666667\n",
    "Most Common Character Frequency: 0.0958904109589041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunk:   6%|â–Œ         | 1893/30824 [00:48<11:27, 42.08it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words, brown\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import ngrams\n",
    "import re\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "\n",
    "nltk_words = set(word.lower() for word in words.words())\n",
    "brown_words = set(word.lower() for word in brown.words())\n",
    "word_freq = FreqDist(word.lower() for word in brown.words())\n",
    "\n",
    "bigram_freq = FreqDist(ngrams(brown.words(), 2))\n",
    "frequency_order = 'etaoinshrdlcumwfgypbvkjxzq'[::-1]\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def create_pattern(substitution_pattern):\n",
    "    return ''.join('*' if char.isupper() else char for char in substitution_pattern)\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def find_matching_words(substitution_pattern):\n",
    "    pattern = create_pattern(substitution_pattern)\n",
    "    regex_pattern = re.compile(f'^{pattern.replace(\"*\", \".\")}$')\n",
    "    return sorted(\n",
    "        (word for word in nltk_words if regex_pattern.fullmatch(word)),\n",
    "        key=lambda x: word_freq[x],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "@lru_cache(maxsize=1000)\n",
    "def validate_mapping(cipher, decipher):\n",
    "    return len(set(cipher)) == len(set(decipher))\n",
    "\n",
    "def evaluate_decryption(decrypted_message):\n",
    "    words = decrypted_message.split()\n",
    "    message_bigrams = list(ngrams(words, 2))\n",
    "    bigram_score = sum(bigram_freq.get(bigram, 0) for bigram in message_bigrams)\n",
    "    english_word_count = sum(1 for word in words if word.lower().replace(\".\", \"\") in brown_words)\n",
    "    english_word_ratio = english_word_count / len(words) if words else 0\n",
    "    char_score = sum(frequency_order.index(char.lower()) for char in decrypted_message if char.isalpha())\n",
    "    return bigram_score, english_word_ratio, char_score\n",
    "\n",
    "def solve_vigenere(text):\n",
    "    chunks = sorted(set(text.replace(\".\", \"\").split(\" \")), key=len, reverse=True)\n",
    "    \n",
    "    def iterate_subs(chunk, index, mapping):\n",
    "        if all(word.islower() for word in chunk):\n",
    "            if not validate_mapping(\"\".join(chunks), \"\".join(chunk)):\n",
    "                return []\n",
    "            copy_text = text\n",
    "            for i, word in enumerate(chunk):\n",
    "                copy_text = copy_text.replace(chunks[i], word)\n",
    "            return [copy_text]\n",
    "        \n",
    "        if index >= len(chunk):\n",
    "            return []\n",
    "        \n",
    "        if chunk[index].islower():\n",
    "            return iterate_subs(chunk, index + 1, mapping)\n",
    "        \n",
    "        matches = find_matching_words(chunk[index])\n",
    "        if not matches:\n",
    "            return []\n",
    "        \n",
    "        possible_answers = []\n",
    "        for match in tqdm(matches, desc=f\"Processing chunk: \", leave=False):\n",
    "            new_mapping = mapping.copy()\n",
    "            valid = True\n",
    "            for c, m in zip(chunk[index], match):\n",
    "                if c.isupper():\n",
    "                    if c in new_mapping and new_mapping[c] != m:\n",
    "                        valid = False\n",
    "                        break\n",
    "                    new_mapping[c] = m\n",
    "            if not valid:\n",
    "                continue\n",
    "            \n",
    "            subbed_chunk = [word.translate(str.maketrans(new_mapping)) for word in chunk]\n",
    "            possible_answers.extend(iterate_subs(subbed_chunk, index + 1, new_mapping))\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "            \n",
    "        return possible_answers\n",
    "    \n",
    "    print(\"Solving cipher...\")\n",
    "    possible_texts = iterate_subs(chunks, 0, {})\n",
    "    best_decryption = None\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    for decrypted_message in tqdm(possible_texts, desc=\"Evaluating decryptions\", unit=\"text\", leave=False):\n",
    "        bigram_score, english_word_ratio, char_score = evaluate_decryption(decrypted_message)\n",
    "        if english_word_ratio == 1:\n",
    "            score = bigram_score + char_score\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_decryption = decrypted_message\n",
    "    \n",
    "    return best_decryption.upper()\n",
    "\n",
    "cipher = \"PRCSOFQX FP QDR AFOPQ CZSPR LA JFPALOQSKR. QDFP FP ZK LIU BROJZK MOLTROE.\"\n",
    "result = solve_vigenere(cipher)\n",
    "print(\"Decrypted message:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SECURITY IS THE FIRST CAUSE OF MISFORTUNE. THIS IS AN OLD GERMAN PROVERB.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.upper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
